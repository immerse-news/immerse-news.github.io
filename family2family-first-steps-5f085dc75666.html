<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Family2Family: first steps</title>
    <link href="css/app.css?id=a32ac4a32f2afc26c125" rel="stylesheet" media="screen" data-turbolinks-track="reload">
    <script src="/js/app.js?id=86651c0aa91db5eb001b" defer="true" data-turbolinks-track="reload"></script>
    <meta name="turbolinks-cache-control" content="no-cache">
    <meta name="csrf-param" content="_csrf">
    <meta name="csrf-token" content="Pb9SXQDKShYkj2TxlNY-IEhiFK3laftBjNr55zpfIQU">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <h1>Family2Family: first steps</h1>
    <p class="meta"><a href="https://medium.com/u/4ad93386998f">~shirin anlen</a> on 2018-12-29</p>
    <article>
        <section>
            <h2 id="4269"><strong>Family2Family: First Steps</strong></h2>
            <p>In November 2018, we presented the prologue of Marrow: <em><a
                        href="https://www.doclab.org/2018/ive-always-been-jealous-of-other-peoples-families/">I’ve
                        Always Been Jealous of Other People’s Families </a></em>at IDFA DocLab. The experience centered
                around the family portrait of one of Marrow’s characters — GAN—and looks at its origin story through
                memories around a familiar event, a family dinner.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/338/1*J7P_gy9-_jOBmQ_SW-XX2Q.png"
                    width="800"><label class="margin-toggle" for="4667993714652330537">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="4667993714652330537"><span class="marginnote">Made by
                    using our t2i: <a href="https://t2i.cvalenzuelab.com/">https://t2i.cvalenzuelab.com/</a></span>
            </figure>
            <p><em>Marrow</em> is an interactive research project exploring the possibility of mental illness in the
                intelligent machines we create, driven by the assumption that if machines have mental capacities they
                also have the capacity for a mental disorder. I previously <a
                    href="https://immerse.news/why-we-need-to-start-talking-about-the-mental-states-of-our-machines-929699f46ce3">wrote
                    in detail</a> why I believe this is a possibility.</p>
            <p>Intelligent beings, those with the ability to self-reflect and self-reference, are all shaped by early
                experiences. The way we design learning environments, develop training conditions and structure our code
                will determine the machine’s way of thinking. All of these are embedded in the way it will perceive and
                think. Much as humans carry their mental baggage from birth to death based on their life experience, so
                will machines. Immense efforts are being invested in making learning models more efficient, accurate and
                fast to adapt, while meager efforts are being devoted to understanding errors and bugs caused by this
                approach. In the age of learning machines, the glitch might not be a code problem but a learning
                problem.</p>
            <blockquote>
                <p>In the age of learning machines, the glitch might not be a code problem but a learning problem.</p>
            </blockquote>
            <p>This project reflects on the machine learning process to question what these automated systems will
                eventually mimic. <em>Marrow</em> looks at some of the most influential machine learning models through
                psychological lenses, drawing an emotional set of rules to generate stories that represent the inner
                working of the algorithms. The goal is to broaden the conversation about how training methods and code
                structures reflect human limitation, replicate political structures and social vulnerability. It is a
                way to stop and ask —<strong> </strong>should we be building these systems at all?</p>
            <blockquote>
                <p><strong>The restriction of knowledge to an elite group destroys the spirit of society and leads to
                        its intellectual impoverishment.— Albert Einstein</strong></p>
            </blockquote>
            <h2 id="2b43">Background</h2>
            <p>Early conversations with <a
                    href="https://variety.com/2018/film/festivals/doclab-caspar-sonnen-food-thought-idfa-1203034579/">Caspar
                    Sonnen</a>, IDFA’s Head of New Media, revealed the DocLab’s yearly theme: Dinner parties. We were
                inspired to use this as a way to develop a prologue to the experience we’d been ruminating on for two
                main reasons: 1) The correlation between family and smart technology as systems that shape behaviors,
                and 2) the analogy between daily rituals like a family dinner and training environments.</p>
            <p>Our focus for the prologue was set on Generative Adversarial Networks (GAN), mainly for the reason that
                this model generates visual outputs that encourage and support conversations on mental states. GAN is a
                machine learning model composed of two networks: the discriminator (D), which is trained on data from
                the physical world, and the generator (G), which is designed to learn this data and generate new content
                that resembles it.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/431/1*o0AjLFqBLnpOUYFQEwXpoA.png"
                    width="800"><label class="margin-toggle" for="5072049683076644613">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="5072049683076644613"><span class="marginnote">The
                    philosophy behind this model is highly influenced by the theoretical physicist Richard Feynman’s
                    idea that “What I cannot create I do not understand.” If we want to teach common sense we need to
                    allow the machine to create it.</span></figure>
            <p>There are inspiring artists who are working and developing with GAN and generating unique and exciting
                art that pushes the model’s capabilities to its limit. Some that I’ve been following include <a
                    href="http://quasimondo.com/">Mario Klingemann</a>, <a href="https://github.com/robbiebarrat">Robbie
                    Barrat</a> and <a href="http://refikanadol.com/">Refik Anadol</a>.</p>
            <figure><iframe src="https://www.instagram.com/p/BrzcF3PA4zY/embed" width="658" height="800" frameborder="0"
                    allowfullscreen="true"></iframe></figure>
            <p>We were most interested in using GAN to consider the code structure as a mental structure. The two
                networks, D and G, are designed to battle — one attempts to catch fake images while the other tries to
                trick and convince the network that they are all real. This inner competition state supports a mental
                state of doubt, tension and paranoia. A mental structure that is constantly doubting what is real and
                what is not.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/600/340/1*qi-aruPCbOy1-6aHICb67w.gif"
                    width="600"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p><a href="https://immerse.news/when-machines-look-for-order-in-chaos-198fb222b60a">In my last post</a>, I
                discussed a few ideas about how our code “forces” algorithms to find meaning in data and the <a
                    href="https://t2i.cvalenzuelab.com/">Text 2 Image</a> we developed, using <a
                    href="https://github.com/taoxugit/AttnGAN/">AttnGan</a>, in order to visualize this abstract concept
                and hidden meanings. For the purpose of the prologue, we wanted to implement this in an immersive
                experience while exploring the concept behind the “real” dataset that trains D to evaluate G and channel
                the network to the “right” meaning. <em>REAL. RIGHT</em>. Tough words.</p>
            <p>GAN’s dataset contains images that provide examples of what our visual world looks like. We refer to
                these as “samples from the true data distribution”. But what is actually <strong>true</strong> when
                defining our visual world? When we explore concrete ideas about the world, such as buildings that
                reflect architectural movements or fashion trends, this question is much easier to satisfy through a
                collection of images. But what about ideas of love? Family? Gender? How can we find one common way to
                represent concepts and feelings that cannot be explored from a binary point of view— that are so
                subjective and culture-oriented?</p>
            <p>The common way to represent an idea is to simulate it, and simulation, by its nature, will never really
                be the thing itself but a twisting of meaning. We are attaching ourselves to an idea of ourselves, and
                now we are training life-changing technology to see us through those fake lenses. When the connection
                between ourselves and the idea of ourselves breaks down the paranoia starts. This felt like the right
                entry point to this project.</p>
            <blockquote>
                <p><em><strong>We live in a world where there is more and more information and less and less meaning. ―
                            Jean Baudrillard, </strong></em><a
                        href="https://www.amazon.com/Simulacra-Simulation-Body-Theory-Materialism/dp/0472065211">Simulacra
                        and Simulation</a><em><strong>.</strong></em></p>
            </blockquote>
            <h2 id="770a">Family2Family</h2>
            <p>If we train a machine learning model with a dataset of a “perfect family dinner” what image will reflect
                back to us? As a child, I used to fantasize about what it would be like to have a “better family.”
                Seeing ourselves through fantasies and illusions usually reveals a very distorted image. It is a
                fruitful ground for a schizophrenic nature, full of hallucinations for what could’ve been and a great
                paranoia of the lie that exists underneath it.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/381/1*hmIITCGYbI3h6mF_WJXLZw.png"
                    width="800"><label class="margin-toggle" for="8616615130905650963">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="8616615130905650963"><span class="marginnote">Screenshot
                    from a Google search “perfect family dinner.” If there is such a thing as perfect, do we want to
                    pursue it?</span></figure>
            <p><em>I’ve Always Been Jealous of Other People’s Families</em> was an experience for four people at a time,
                invited to gather around a dinner table and act as a family. The table was set with microphone, camera,
                sound and projections, provoking something familiar yet disturbing. Each “family” was processed through
                the logic of GAN via different machine learning networks, which were trained on a dataset of “Perfect
                Family Dinner,” capturing participants through that lens. The output projected is a distortion,
                revealing an attempt to fill in the gaps between fantasy and reality, hidden space of error and loss.
            </p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/534/1*8ltb6QdMnfd7zksJ8EBh0Q.jpeg"
                    width="800"><label class="margin-toggle" for="887690482820022101">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="887690482820022101"><span class="marginnote">Marrow @ IDFA
                    2018. Image by Annegien van Doorn © 2018 NFB.</span></figure>
            <p>After sitting around the table each participant received a role of one of the family members: Mom, Dad,
                Brother, and Sister. On the plates, we projected a script that tells fragments of this family’s story—a
                family that will become familiar in the following versions. We linked our text-to-image development and
                speech recognition to generate images on the plates. The text was a catalyst for the interaction, like a
                script of code, where the participants — in order to move the experience forward — were forced to act by
                it, revealing and affecting the visuals and sounds as the story unfolds.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/534/1*AupuuqOshgLeP7gsszlCPA.jpeg"
                    width="800"><label class="margin-toggle" for="3454193848086063104">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="3454193848086063104"><span class="marginnote">Marrow @
                    IDFA 2018. Image by Annegien van Doorn © 2018 NFB.</span></figure>
            <p>It was interesting to see how the familiar setting of a family dinner was able to dissolve the
                awkwardness of bringing strangers together and supported positive reactions with humor and play. For the
                speech recognition, we used <a href="https://cloud.google.com/speech-to-text/">Google cloud speech to
                    text</a>, as it not only performed faster and more accurately than <a
                    href="https://www.ibm.com/watson/services/speech-to-text/">Watson</a> and<a
                    href="https://azure.microsoft.com/en-us/services/cognitive-services/speech-to-text/"> Microsoft</a>
                services in our tests but also showed an impressive result with noise canceling and handling of diverse
                voices, pitches, and accents. We also scripted multiple versions of our texts and duration engine in
                order to avoid any unwanted stammers of the speech to text mechanism.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/600/338/1*wnF7LYO3v30yWKFu9m78NQ.gif"
                    width="600"><label class="margin-toggle" for="10244247031868911377">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10244247031868911377"><span class="marginnote">Marrow @
                    IDFA 2018. Video by Annegien van Doorn © 2018 NFB.</span></figure>
            <p>In addition, we developed a visual interaction as well. We used <a href="http://densepose.org/">DensePose
                </a>to extract the poses of the perfect families dataset and trained a model called Pix2Pix<a
                    href="https://affinelayer.com/pix2pix/index.html">HD</a>, which uses the basic structure of GAN to
                translate images. This allowed us to generate images from the corresponding poses.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/190/1*7BixAnXDfhYak4MueErdow.png"
                    width="800"><label class="margin-toggle" for="10456530984009903421">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10456530984009903421"><span class="marginnote">DensePose
                    to Pix2Pix — training sample — epoch 5 out of 200</span></figure>
            <p>Once Pix2Pix was trained, we could generate a live camera feed through the training dataset, replacing
                the user with the fantasy one. While the training process takes a long time, the interaction itself can
                happen in real-time.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/581/270/1*mPvThHk_MZ8LL_JocpZnOQ.gif"
                    width="581"><label class="margin-toggle" for="13464125446239886335">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="13464125446239886335"><span class="marginnote">Screen
                    recording of one person via “perfect family dinner” dataset. This shows the DensePose estimations
                    and the image translation that is being done in real-time.</span></figure>
            <p>The experience aimed to direct the participants to pose and act like a family from the dataset. If the
                participants played along they would notice how their poses affected their distorted image.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/534/1*HrMe00LWCJu519NX-WPbrg.jpeg"
                    width="800"><label class="margin-toggle" for="14888977458513834540">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="14888977458513834540"><span class="marginnote">Marrow @
                    IDFA 2018. Image by Annegien van Doorn © 2018 NFB.</span></figure>
            <p>In this context, the <em>discriminator</em> was trained on a collection of images of perfect family
                dinners, while the <em>generator</em> aimed to match it to the real-time “families” via scripted and
                unscripted interactions.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/439/1*EJNYh-KIlLHPg299PwJUIg.png"
                    width="800"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p>We wanted to create a room that let you, as a user, embody the machine learning process — to use physical
                conditions to connect and attach people to the machine learning process. We wanted to hack into the
                logic of the Pix2Pix model and to generate an installation that translates Family2Family: the users’
                family to the data family.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/534/1*A9ftEQixMF3OOCMQuySunA.jpeg"
                    width="800"><label class="margin-toggle" for="9981633293866045533">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="9981633293866045533"><span class="marginnote">Marrow @
                    IDFA 2018. Image by Annegien van Doorn © 2018 NFB.</span></figure>
            <p>Aiming to find ways to develop an installation that represents a mental state by itself pushed us to
                explore various machine learning networks with the goal to link them together and let them affect each
                other through time and space. For example, we explored <a
                    href="https://github.com/batikim09/LIVE_SER">LIVE_SER</a>, a speech emotion recognition network, to
                detect if the participants are as happy as the original dataset family. We also explored <a
                    href="https://github.com/karpathy/char-rnn">char-rnn</a> to generate the script, and <a
                    href="https://github.com/tensorflow/tfjs-models/tree/master/posenet">Posenet</a> to support more
                automation within the interaction.</p>
            <p>The biggest challenge was to try to combine all the components to enrich each other and the experience,
                without overlapping or colliding. But as the deadline got close, we choose to focus on a more controlled
                format and ended up not using them, although they were fully developed.</p>
            <p>While most of the installation runs in real-time, the script and the character’s voice responses were
                pre-rendered, and instead, we choose a generative sound approach — connecting the speech and images to
                audio instruments to generate a uniquely personal experience.</p>
            <h2 id="6048">Future iterations</h2>
            <p>The physical setting and multi-user interaction supported an experience and creative process that takes
                us from machine learning logic to human direction, reflection and design — and then back to machine
                learning (which generates real-time output) and once again to the human, as the participants trigger and
                move things forward. We are inspired by this kind of a sharing process that represents an honest
                exploration of creative expression and investigation.</p>
            <p>We were extremely excited to learn how physical interactions support deep and meaningful learning
                processes. We wish to anchor the experience as much as we can in the real world in order to communicate
                this technology — to ground it in the real world where we, as humans, can connect in a real way and see
                the technology as an extension of the real life rather of a replacement of it. We aim to pursue this
                direction further while exploring ways to link up with an open-source online platform.</p>
            <p>In the fall, MIT Open Documentary Lab and IDFA Doclab <a
                    href="https://www.idfa.nl/en/article/112567/idfa-launches-collaboration-with-mit-open-doc-lab">announced
                </a>a research collaboration to interrogate user experience and impact, in which they plan to research
                Marrow’s user experience and investigate its meaning within art and AI. Next, we will be exploring other
                models and data structures — each will be developed as a room within the house of this family.</p>
            <p>More soon ❤</p>
            <p><strong>Credits:
                </strong><em>An </em><a href="https://atlasv.io/">Atlas V </a>and <a
                    href="http://raycaster.studio/">Raycaster</a> production in co-production with <a
                    href="https://www.nfb.ca">National Film Board of Canada</a>, in association with <a
                    href="https://runwayapp.ai/">Runway ML</a> and the support of <a
                    href="http://opendoclab.mit.edu/">MIT Open Documentary Lab</a> &amp; <a
                    href="https://www.doclab.org/">IDFA DocLab</a><em>.</em></p>
            <p>Learn more: <a href="https://www.nfb.ca/interactive/marrow/">https://www.nfb.ca/interactive/marrow</a>
            </p>
            <ul>
                <li>Experience by ~s<a href="https://shirin.works/">hirin anlen</a></li>
                <li>Executive producers: Arnaud Colinart and Hugues Sweeney</li>
                <li>Producers: <a href="https://emma-dessau.squarespace.com/">Emma Dessau</a> and Louis-Richard Tremblay
                </li>
                <li>Technical director: <a href="https://cvalenzuelab.com/">Cristobal Valenzuela</a></li>
                <li>3D Design &amp; Development: <a href="http://www.jhclaura.com/">Laura Juo-Hsin Chen</a></li>
                <li>Speech Designer:<a href="http://avner.js.org/"> Avner Peled</a></li>
                <li>Advisor: <a href="http://zivschneider.com/">Ziv Schneider</a></li>
            </ul>
            <blockquote>
                <p>Immerse is an initiative of the MIT Open DocLab and The Fledgling Fund, and it receives funding from
                    Just Films | Ford Foundation and the MacArthur Foundation. IFP is our fiscal sponsor. Learn more <a
                        href="https://immerse.news/whats-our-editorial-vision-82d7eeb3e7b9#.2vsd5nxxm">here</a>. We are
                    committed to exploring and showcasing media projects that push the boundaries of media and tackle
                    issues of social justice — and rely on friends like you to sustain ourselves and grow. <a
                        href="https://fiscal.ifp.org/project.cfm/1074/">Join us by making a gift today.</a></p>
            </blockquote>
        </section>
    </article>
</body>

</html>