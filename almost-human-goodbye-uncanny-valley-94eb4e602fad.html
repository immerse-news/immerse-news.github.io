<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Almost Human: Goodbye Uncanny Valley</title>
    <link href="css/app.css?id=a32ac4a32f2afc26c125" rel="stylesheet" media="screen" data-turbolinks-track="reload">
    <script src="/js/app.js?id=86651c0aa91db5eb001b" defer="true" data-turbolinks-track="reload"></script>
    <meta name="turbolinks-cache-control" content="no-cache">
    <meta name="csrf-param" content="_csrf">
    <meta name="csrf-token" content="q7YiTScfbFGtj8RKzCpau1MSwPQ2mvHm1TblcGyKKf8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <h1>Almost Human: Goodbye Uncanny Valley</h1>
    <p class="meta"><a href="https://medium.com/u/fd2d1693949">Or Fleisher</a> on 2019-02-02</p>
    <article>
        <section>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/266/1*MJdZ2FmJ_gTfTl4x1iYH2w.png"
                    width="800"><label class="margin-toggle" for="6537168585842615087">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="6537168585842615087"><span class="marginnote">Image
                    credit: <a href="https://alteredqualia.com/xg/examples/sunshine.html">Sunshine by
                        AlteredQualia</a></span></figure>
            <h2 id="4360">Holograms, realism, or simply goodbye uncanny valley</h2>
            <p>As early as the 1980s, popular cinema and television began depicting holograms of the future. These
                transparent blue figures have been a great influence on our perceptual model of <em>what holograms
                    should look like</em>. Nowadays, innovation in machine learning, computer graphics, and hardware are
                paving the way for holographic content to become mainstream, and, yet, some of the questions I still ask
                myself are: Why are we so obsessed with realism? What is the archival benefit of documenting humans in
                3D? What is the connection between holograms and personal assistants?</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/359/1*BPkXo63ADdUHGdApqSofCQ.png"
                    width="800"><label class="margin-toggle" for="324617142887668195">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="324617142887668195"><span class="marginnote">Image credit:
                    <a href="https://fractalfantasy.net/#/4/uncannyvalley">AlteredQualia / Branislav Ulicny Uncanny
                        Valley WebGL experience</a></span></figure>
            <p>Let&#39;s examine the idea of the uncanny valley. The <em>uncanny valley </em>term was coined by <a
                    href="https://en.wikipedia.org/wiki/Masahiro_Mori_(roboticist)">Masahiro Mori</a> and presented in
                <a href="https://en.wikipedia.org/wiki/Jasia_Reichardt">Jasia Reichardt</a>’s book <em><a
                        href="https://www.amazon.com/Robots-Fiction-Prediction-Jasia-Reichardt/dp/014004938X">Robots:
                        Fact, Fiction, and Prediction</a></em><em>.</em>
            </p>
            <blockquote>
                <p>In aesthetics, the <strong>uncanny valley</strong> is a hypothesized relationship between the degree
                    of an object’s resemblance to a human being and the emotional response to such an object <a
                        href="https://philpapers.org/rec/MACTUA-5">[Karl F. MacDorman &amp; Hiroshi Ishiguro]</a>.</p>
            </blockquote>
            <p>This idea is perhaps best suited to describe not a phenomenon but an era, which we are arguably
                transitioning out of. With <em>smarter </em>software becoming ubiquitous in almost every aspect of our
                lives, we are (consciously or not) building things that behave and react more like us. When we emphasize
                function over imitative visuals we tend to avoid that uncomfortable “uncanny valley” feeling. Is
                Amazon’s Alexa or Google Home uncanny? I would argue not since speech generation has become convincing
                enough paired with pure functional use:</p>
            <p><em>Me: Alexa, put something on my shopping list
                    Alexa: Ok.</em></p>
            <p>Perhaps <em>the</em> <em>uncanny valley </em>is a result of the transition from 2D to 3D imagery and will
                no longer be needed once holograms become <em>convincing enough, </em>alongside playing a functional
                role in our lives.</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F237568588%3Fapp_id%3D122963&amp;dntp=1&amp;url=https%3A%2F%2Fvimeo.com%2F237568588&amp;image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F660027609_1280.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=vimeo"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="11271620826116805703">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="11271620826116805703"><span class="marginnote">Alan Warburton’s excellent visual
                    essay, “Goodbye Uncanny Valley,” provides some history on experimental computer graphics</span>
            </figure>
            <p>Our lives are surrounded by interfaces, apps, physical signage, and, more recently, voice interfaces such
                as Alexa and Google Home. These interfaces are meant to serve a specific function in our day-to-day
                lives but, more often than not, they look, feel, and sound nothing like us. With more and more demand
                for computer-generated imagery (CGI) in recent years, and the popularization of platforms such as
                augmented and virtual reality, computer games, and interactive filmmaking, it is clear there is a
                potential for new human-computer interfaces that also resemble us visually.</p>
            <blockquote>
                <p>There is a potential for new human-computer interfaces that also resemble us visually.</p>
            </blockquote>
            <p>To create that, companies, and artists are exploring various forms of 3D capturing meant to replicate the
                human element, in new and compelling ways beyond two-dimensional pixels. These tools are what forms the
                basis for volumetric capturing, a collection of techniques for capturing three-dimensional humans.</p>
            <p>These tools are not born in a vacuum. Some are born from a product vision, such as <a
                    href="https://www.intel.com/content/www/us/en/sports/sports-overview.html">Intel’s Replay technology
                    for 3D sports</a>, which engages sports fans in a new way by allowing them to replay a move from
                different angles. Other tools are born from computational aesthetic explorations, such as Scatter’s <a
                    href="http://depthkit.tv">Depthkit</a>, which was initially developed in order to create <a
                    href="http://cloudsdocumentary.com">CLOUDS</a>, a volumetric documentary about creative uses of
                software. One thing all of the tools share is the visual, technical, and anthological exploration of how
                to represent and document real humans in 3D space.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/545/232/1*lDeEabsvWapbqVG1CX4EjA.gif"
                    width="545"><label class="margin-toggle" for="9959229370521302706">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="9959229370521302706"><span class="marginnote">Image
                    credit: <a href="https://www.intel.com/content/www/us/en/sports/technology/true-view.html">Intel
                        TrueView</a></span></figure>
            <p>During <a href="https://orfleisher.com/volume">my thesis</a> research in <em><a
                        href="https://tisch.nyu.edu/itp">ITP</a></em>, I focused on the possibilities of using machine
                learning to reconstruct archival and historical footage in 3D. The idea of using machine learning was
                born out of a desire to look back into more than 200 years of visual culture (i.e. 2D photography) and
                speculate about how we can bridge the growing gap between 2D and 3D.</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F270190550%3Fapp_id%3D122963&amp;dntp=1&amp;url=https%3A%2F%2Fvimeo.com%2F270190550&amp;image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F702415671_1280.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=vimeo"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="13945536073815538751">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="13945536073815538751"><span class="marginnote">Volume — NYU ITP thesis
                    presentation on Vimeo</span></figure>
            <p>Recently, Apple has included a depth sensor into the new iPhones. Facebook now lets you post 3D photos to
                your wall. And Snapchat has augmented reality facial filters. These are only a few examples of recent
                spatial interfaces invading our daily lives. My research has led me to believe that we are in a moment
                of acute awareness of the transition from 2D to 3D. Even though some might argue that we have already
                entered the 3D era, it seems to me that this is only the tip of the iceberg. Look back at the transition
                from black and white to color, from analog film to digital, and, through that lens, think about the
                transition from 2D to 3D. This is nothing short of a revolutionary cultural moment.</p>
            <p>For example, today, black and white imagery is used to symbolize authenticity and age, but before the
                widespread availability of color photography, it was considered a representation of current reality. So
                how will we look back at two-dimensional media a hundred years from now? Will it only be a testimony of
                our past, or can we bridge the gap by using new technologies?</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/311/1*pHbgCYlctMtzVQ68OjbY8g.png"
                    width="800"><label class="margin-toggle" for="15490475056368922202">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="15490475056368922202"><span class="marginnote">The
                    evolution of photography, Image credit: <a href="https://orfleisher.com/volume">Volume</a></span>
            </figure>
            <p>Machine learning advancements have made it possible to relive films in a way that generates compelling
                results. For example, as a part of my thesis research, together with S<a
                    href="https://medium.com/@s.h.i.r.i.n">hirin Anlen</a>, we reconstructed a scene from <em>Pulp
                    Fiction </em>in augmented reality. This example used machine learning to separate the characters
                from the background and generate volumetric figures that are then used in the augmented reality scene.
            </p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FiwJt4DM6mJA%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DiwJt4DM6mJA&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FiwJt4DM6mJA%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="3411349805084827442">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="3411349805084827442"><span class="marginnote">Inside Pulp Fiction on
                    YouTube</span></figure>
            <h2 id="a47b">Volumetric capturing?</h2>
            <p>There are a wide variety of techniques, which range from laser scanning (also referred to as LIDAR
                scanning), infrared sensors (a notable example is <em>Microsoft’s Kinect camera</em>) and, most
                recently, the use of machine learning and convolutional neural networks to reconstruct a 3D object from
                2D images (as shown in the above <em>Pulp Fiction</em> example). These methods all have roots in
                different fields such as defense, robotics, and topology but are now being used more and more for art,
                entertainment, and media.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/450/1*upWmObxj7U2ZjxK6eBg6Kw.jpeg"
                    width="800"><label class="margin-toggle" for="15963664926695991810">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="15963664926695991810"><span class="marginnote">Image
                    credit: <a href="https://vimeo.com/104385260">Marshmallow Laser Feast MEMEX | Duologue on
                        Vimeo</a></span></figure>
            <h2 id="fc8b">Computational humans? Alexa gets a body</h2>
            <p>Innovation in machine learning doesn’t only affect the fidelity of the 3D acquisition and rendering
                process but also provides ground for procedurally generated facial expressions and dialog which bears an
                amazing resemblance to us, the human counterpart. For example, Hao Li, Director of the Vision and
                Graphics Lab at USC and founder of Pinscreen, published research showing the ability of a Generative
                Adversarial Networks (GANs) to reconstruct a 3D facial model from a 2D image, which can then be
                puppeteered.</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2F_JMx5VN1eeM%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3D_JMx5VN1eeM&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2F_JMx5VN1eeM%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe></figure>
            <p>Popular entertainment is also taking note. In order to create facial expressions behind Thanos in
                Marvel’s <em>Avengers</em> films, VFX studio, Digital Domain, created machine-learning-driven software,
                <a href="https://www.engadget.com/2018/08/18/avengers-thanos-ai/">called Masquerade</a>, that aids
                artists in creating compelling facial expressions. Imagine <a
                    href="https://www.youtube.com/watch?v=FIa4JJLfzI0">Google’s Duplex demo</a>, combined with the
                facial expressions produced by the Masquerade software — personal assistants are given a big facelift,
                well, quite literally.
            </p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/422/1*-JR0l1Aule6ThD3cfMovCA.jpeg"
                    width="800"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p>After watching some of these tech demos, I found myself engaged in a conversation about the nature of
                personal assistants with <a href="http://drorayalon.com">Dror Ayalon</a>. An interesting point arose in
                that we are experiencing a transition from personal assistants morphing into personal companions. The
                idea of embodying that voice that keeps our Amazon shopping lists, turns the lights on, and sets a timer
                during our spontaneous decision to cook is yet another step towards Alexa getting a body and becoming
                <em>almost human</em>.
            </p>
            <blockquote>
                <p>We are experiencing a transition from personal assistants morphing into personal companions.</p>
            </blockquote>
            <p>Films have already imagined this idea, and it seems there is still a way to go before we can get to the
                vision portrayed in <em>Her </em>where Alexa sounds like Scarlett Johansson and helps you win a
                holographic video game.</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F97740427%3Fapp_id%3D122963&amp;dntp=1&amp;url=https%3A%2F%2Fvimeo.com%2F97740427&amp;image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F478220369_1280.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=vimeo"
                    width="800" height="434" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="10967268799695472952">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="10967268799695472952"><span class="marginnote">Hologram sequences from the film
                    Her</span></figure>
            <p>There is an argument to be made that Alexa doesn’t necessarily have to look like us. Take, for example,
                Anki’s Vector robot, which provides a very compelling experience without some of the visual human
                features; it feels like a physical embodiment of Pixar’s communication of emotion through sounds and
                facial expressions.</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F293390863%3Fapp_id%3D122963&amp;dntp=1&amp;url=https%3A%2F%2Fvimeo.com%2F293390863&amp;image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F730016226_640.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=vimeo"
                    width="640" height="360" frameborder="0" allowfullscreen="true"></iframe></figure>
            <p>That said, a human representation could stretch beyond novelty and utility into something that resembles
                a relationship, not just <em>“Order more toilet paper.”</em></p>
            <h2 id="28d6">Alexa, stop!</h2>
            <p>All this said and done, it seems spatial computing, volumetric capturing, and voice interfaces are
                becoming interconnected and are on a track to disrupt our perception of the <em>uncanny valley.
                </em>Personally, I am really excited about the possibilities in software interfaces becoming, well, a
                little more like us, and a little less like this.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/480/454/1*PJxJMXDh7acFCOkv7KcSPA.gif"
                    width="480"><label class="margin-toggle" for="8307175172537909345">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="8307175172537909345"><span class="marginnote">Scrolling —
                    From <a
                        href="https://giphy.com/gifs/scrolling-scroll-hasan-minhaj-vvWhQsVAFkyisScAsM">Giphy</a></span>
            </figure>
            <blockquote>
                <p><em>Immerse</em> is an initiative of the MIT Open DocLab and The Fledgling Fund, and it receives
                    funding from Just Films | Ford Foundation and the MacArthur Foundation. IFP is our fiscal sponsor.
                    Learn more <a
                        href="https://immerse.news/whats-our-editorial-vision-82d7eeb3e7b9#.2vsd5nxxm">here</a>. We are
                    committed to exploring and showcasing media projects that push the boundaries of media and tackle
                    issues of social justice — and rely on friends like you to sustain ourselves and grow. <a
                        href="https://fiscal.ifp.org/project.cfm/1074/">Join us by making a gift today.</a></p>
            </blockquote>
        </section>
    </article>
</body>

</html>