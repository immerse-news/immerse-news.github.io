<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Will AI Editing Change How We See?</title>
    <link href="css/app.css?id=a32ac4a32f2afc26c125" rel="stylesheet" media="screen" data-turbolinks-track="reload">
    <script src="/js/app.js?id=86651c0aa91db5eb001b" defer="true" data-turbolinks-track="reload"></script>
    <meta name="turbolinks-cache-control" content="no-cache">
    <meta name="csrf-param" content="_csrf">
    <meta name="csrf-token" content="SIK97pLZDBMPmPctOhFYRF21w61yBZbYkOqbTv_z_Sk">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <h1>Will AI Editing Change How We See?</h1>
    <p class="meta"><a href="https://medium.com/u/b16ee4706fcc">Deniz Tortum</a> on 2022-02-23</p>
    <article>
        <section>
            <h3 id="b8f3">Runway’s pivot from machine learning to browser-based editing software sparks this reflection
                on the future of video editing</h3>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/478/1*D6kDS4OQBnngfHyev3wWqQ.png"
                    width="800"><label class="margin-toggle" for="14882607086399168519">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="14882607086399168519"><span class="marginnote">Screenshot
                    from Runway’s browser-based video editing platform, using the inpainting function to remove a
                    basketball from the shot</span></figure>
            <p>A couple years ago, I used Generative Adversarial Networks (GAN) to create a sequence that had previously
                only existed in my mind: a turtle trying to evolve into a different species, but it was stuck somewhere
                in between, each form resembling a distant relative of a turtle. At first, I didn’t know what tool could
                help me make this sequence. Scrolling through my social media feeds, I came across videos where images
                transition into each other seamlessly. This technique would have been ideal for the sequence in mind but
                I didn’t have the technical knowledge to create it myself.</p>
            <p>That’s when I came across Runway, a user-friendly machine learning software that gave artists the tools
                to create AI-generated imagery. I created the initial turtle and morphed the image into many different
                variations by changing some of the parameters of the network to create a “latent walk”, a journey in the
                vector space of the image network. It resulted in this brief shot from <em>Our Ark</em> (2021):</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FaOJn4g1hMAw%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DaOJn4g1hMAw&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FaOJn4g1hMAw%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="2867668657368423851">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="2867668657368423851"><span class="marginnote">Our Ark (2021, co-directed by
                    Deniz Tortum &amp; Kathryn Hamilton)</span></figure>
            <p>With this tool, you could generate images and image sequences, train your own data sets, experiment with
                the working logic of GANs and visualize the impact of high-dimensional spaces.</p>
            <p>Last <a href="https://runwayml.com/blog/runway-raises-35-series-b/">December</a>, Runway raised $35
                million dollars in their Series B funding. Then, the company pivoted into a browser-based video editing
                platform. This was a curious move — isn’t video-editing software already a saturated field? How is
                machine learning related to video editing?</p>
            <p>This move provides some insights here into where video production is heading in the near future. In
                gaming, instead of running certain games on your own device, you can now “stream” games which run on
                distant servers via cloud gaming subscription services. Similarly, moves like Runway’s indicate that
                video editing is moving into the cloud. Normally you’d need a strong editing computer with fast GPUs to
                edit and composite 4K footage. With cloud editing, strong computers in a remote server room would do the
                heavy work as you edit on a browser with a simple computer. A <em>cloud-editor</em> could be someone who
                edits and does visual effects (VFX) work anywhere with any computer.</p>
            <p>The other insight concerns the confluence of machine learning and VFX. Runway is just one team that is
                applying its experience with ML into the field of VFX. For example, with the datasets they trained,
                image segmentation or rotoscoping (the process of separating objects or characters from the background)
                is possible <a
                    href="https://research.runwayml.com/building-a-web-based-real-time-video-editing-tool-with-machine-learning">with
                    just a few clicks</a>. Until recently, these actions had to be done frame by frame. It is also
                possible to easily remove an object with a technique called <em>inpainting</em>. The computer
                automatically paints the background as if the object or the person never existed. For Cristóbal
                Valenzuela, CEO of Runway, AI “is enabling [users] to edit and generate video in ways that were
                unimaginable before.”</p>
            <p>The film editing process changed a lot with the transition to digital, and will change drastically again
                as we start to use AI tools. Runway is not the only such project. For example:</p>
            <ul>
                <li><a href="https://medium.com/janbot/jan-bots-step-by-step-822b831d0402">Jan Bot</a>, an editing
                    algorithm that scrapes the Eye Film Institute’s archive and automatically creates videos</li>
                <li><a href="https://www.kasparai.com/">Kaspar</a>, an AI assistant editor that tags, pulls selects and
                    creates metadata</li>
                <li>Tools to edit video interviews by just editing the text, such as <a
                        href="https://reduct.video/">Reduct</a></li>
                <li>Text-to-video, such as OpenAI’s <a href="https://openai.com/blog/clip/">Clip</a> (and <a
                        href="https://www.ziaxaza.com/">this music video</a> created with it)</li>
                <li>Videos that are generated by AI, such as the work of Casey Raes, Anna Ridler and Refik Anadol.
                    Anadol, in particular, is popularizing the term “latent cinema” to describe this genre, but the term
                    is also <a href="https://uspto.report/TM/88536609/ALW20200225074539/">trademarked to his company</a>
                </li>
            </ul>
            <p>All these changes prompt us to reframe the practice of creating moving images. In her<em> </em>“State of
                Cinema 2021” <a href="https://sabzian.be/text/state-of-cinema-2021-1">essay</a>, Nicole Brenez, curator
                of Cinémathèque Française, writes that “technical images have invaded the universe.” These images are
                created by computers, algorithms, technology, and mathematics. They are images that explain and operate
                on society. We have been using the tools of cinema, especially editing, to study and analyze these
                images by forming new relations between them. But now we are going beyond technical images — what about
                technical cuts? Editing decisions that are made by artificial intelligence? How should we deal with
                these computer-generated relations between images?</p>
            <p>Scrolling through my algorithmically-generated social media feeds, I realize that what I’m looking at is
                a collection of technical cuts. Everyday, we are dealing with image sequences that are generated for us,
                and whose structuring logic we can’t understand. The relations these images have with each other is
                opaque. We try to make sense of them all day every day. We have already stepped into the world of
                AI-editing.</p>
            <p><em>For more news, discourse, and resources on immersive and emerging forms of nonfiction media, </em><a
                    href="https://immerse.news/subscribe-to-our-monthly-newsletter-aa39eb11ccc2">sign up</a><em> for our
                    monthly newsletter.</em></p>
            <p>Immerse<em> is an initiative of the MIT Open DocLab and Dot Connector Studio, and receives funding from
                    Just Films | Ford Foundation, the MacArthur Foundation, and the National Endowment for the Arts. The
                    Gotham Film &amp; Media Institute is our fiscal sponsor. Learn more </em><a
                    href="https://immerse.news/whats-our-editorial-vision-82d7eeb3e7b9">here</a>. We are committed to
                exploring and showcasing emerging nonfiction projects that push the boundaries of media and tackle
                issues of social justice — and rely on friends like you to sustain ourselves and grow. <a
                    href="https://fiscal.thegotham.org//project.cfm/1074/">Join us by making a gift today</a><em>.</em>
            </p>
        </section>
    </article>
</body>

</html>