<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Your Future Needs You</title>
    <link href="css/app.css?id=a32ac4a32f2afc26c125" rel="stylesheet" media="screen" data-turbolinks-track="reload">
    <script src="/js/app.js?id=86651c0aa91db5eb001b" defer="true" data-turbolinks-track="reload"></script>
    <meta name="turbolinks-cache-control" content="no-cache">
    <meta name="csrf-param" content="_csrf">
    <meta name="csrf-token" content="ETrhQpM5ilPkAWfpiI_MOPsAn5G9DKNrQp46pJ9-ovw">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <h1>Your Future Needs You</h1>
    <p class="meta"><a href="https://medium.com/u/78d2f9592a40">Story Teller From The Future</a> on 2018-06-25</p>
    <article>
        <section>
            <h3 id="d18e">Why AI is not the future, WE are.</h3>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/352/0*7xTn-hJXPxIBIrcX" width="800"><label
                    class="margin-toggle" for="200568024173538993">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="200568024173538993"><span class="marginnote">The Storyteller from the Future
                    (a.k.a Karen Palmer) developing algorithms covertly. Photo Kevin Barry</span></figure>
            <p><em>[TRANSMISSION LOG #7054—Origination year 2048: Destination 2018]</em></p>
            <p>If you are receiving this broadcast, you are the intended receiver! I am the Storyteller from the Future
                and I have come back to enable you to survive what is to come.</p>
            <p>I am transmitting this stream of data for you today as this is a pivotal moment in your planet’s
                timeline. Your planet is experiencing unprecedented global social unrest grounded in race and the
                economy. Simultaneously, your world is on the cusp of a new frontier of immersive technological and
                media experiences that will not only entertain but influence your kind in an unprecedented manner:
                psychologically, consciously, and neurologically.</p>
            <p><em>Why should I believe you?</em> Is the question I hear you asking.</p>
            <p>As a result of prior incidents when details from my time have been transmitted back into yours,
                stipulations have been placed upon what can be communicated. But I will just give you an insight to
                where your driverless cars may be driving some of you to.</p>
            <p>In the future, automated technology, facial recognition, and AI will often be exploited for suppressive
                means. For example, if you have an outstanding warrant, even minor, when you get into your Uber
                driverless vehicle, it will be secured, locked down, and you will be driven directly to the associated
                police station.</p>
            <p>You see, the future technology will be used as an extension of oppression and racism. You have seen the
                beginning of this recently in your news with the advent of Amazon selling facial recognition software,
                Rekognition, to law enforcement. While the press <a
                    href="https://www.engadget.com/2018/06/22/amazon-employees-letter-face-recognition/">covered the
                    story</a> of an open letter by employees and shareholders demanding that Amazon stop…well, this
                story will quietly disappear. Such special relationships between global tech corporations and law
                enforcement will only happen now behind closed doors in order to prevent resistance. This will mark the
                beginning of a new era of unethical and inequitable technology and governmental partnership.</p>
            <p>But let me not deviate from my objective in this data stream. Let me start at the beginning: My past!
                Your present!</p>
            <p>Wait, they are coming for me! The perimeter of this safe quadrant has been compromised …</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fw.soundcloud.com%2Fplayer%2F%3Furl%3Dhttp%253A%252F%252Fapi.soundcloud.com%252Ftracks%252F148736915%26show_artwork%3Dtrue&amp;url=https%3A%2F%2Fsoundcloud.com%2Fsoundeffectsforfree%2Fpolice-siren-sound-effect&amp;image=http%3A%2F%2Fi1.sndcdn.com%2Fartworks-000078957675-vi2c30-t500x500.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=soundcloud"
                    width="800" height="166" frameborder="0" allowfullscreen="true"></iframe></figure>
            <p><em>[TRANSMISSION LOG #7054: Terminated]</em></p>
            <p><em>[TRANSMISSION LOG #7055—Origination Year 2049: Destination 2018]</em></p>
            <p>It has taken almost a year to re-establish a secure transmission frequency.</p>
            <p>At this time on your planet, I am known as Karen Palmer, a multidisciplinary immersive filmmaker. While I
                have been projected back into your time, my consciousness remains in my time and guides my current
                physical being. Therefore, I do not experience the world in quite the same way you do. As the
                Storyteller from the Future, I apprehend this reality beyond conventional perception. At risk of my
                safety, I feel that my responsibility is to enable you to experience that, too.</p>
            <p>Imagine, if you will, that we are existing in the most realistic video game simulation known to (wo)man.
                What you perceive as yourself is a pre-selected avatar. Your operating system (let’s call it your
                “belief system”) is an amalgamation of your data — a.k.a your experiences, interpreted via your personal
                sensors, more commonly known as your eyes and ears.</p>
            <p>Now imagine that you could re-program and update your software to redefine your reality and hone your
                perception. This can be done and that’s why I created <em>RIOT</em>.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/450/1*bBKeEV60yZfIsE8knoryHQ.jpeg"
                    width="800"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p><em>RIOT</em> is an emotionally responsive film which uses facial recognition and artificial intelligence
                technology to enable players to navigate through a dangerous riot. You are confronted by a riot cop:
                Respond with fear and the film goes in one direction. Respond with anger and it goes in another. I have
                created this because in the time my body inhabits, most of the countries are now police states. I want
                you to know what it is going to feel like for you and your families to have a police presence
                everywhere, in the hope that it primes your subconscious to start to resist!</p>
            <p><em>The RIOT</em> sensory storytelling experience enables participants to become aware of their
                subconscious behaviour and enables them to consciously build new neurological pathways in their brains,
                to override automatic behaviourial responses and create new ones. The underlying architecture of the
                experience is to shift individual’s perception by rewiring their minds, and it’s working.</p>
            <p>In 2016, my projected consciousness developed the <em>RIOT </em>prototype with Crossover Labs, Sheffield
                University for Festival of the Mind, The National Theatre Immersive Storytelling Studio and Dr. Hongying
                Meng, a Senior lecturer at Brunel University London’s Computer Science and Engineering Department.</p>
            <p>The project was inspired by my own frustration as I watched the Ferguson riots unfold on social media,
                having seen yet another young black man murdered by the police. I wanted people to experience the untold
                story of real people in a situation who see no other option left open to them. To tell a story of not
                just being in a riot, but what has led to it and the intensity of fighting for injustice even when your
                very life is at stake. To make you feel your own fear and master it.</p>
            <p>As a participant in the film, how you think you would respond may not be how you actually would. So, you
                could go back into the experience and consciously change your behaviour in order to build new
                neurological pathways in your brain. People have returned to participate in the film and responded
                differently based on their awareness of their subconscious behaviour.</p>
            <p>Research in neuroscience and cognitive psychology has shown that stories are typically more effective
                than rational arguments at changing minds. That’s why we selected storytelling as the most obvious form
                to connect with your people at this time. In fact, the power of these new forms of art and tech are only
                becoming apparent. MIT has carried out research that confirms that your brain cannot decipher the
                difference between VR and reality.</p>
            <p>I am part of this revolution in experiential storytelling. But don’t worry: I am one of the good guys.
            </p>
            <p>Here, in a communication ritual you know as a “TEDx talk,” I explain more.</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FRw8gLEkFdSw%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DRw8gLEkFdSw&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FRw8gLEkFdSw%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe></figure>
            <p>The current second iteration, <em>RIOT 2,</em> starts with an actor dressed as a riot police officer
                whose objective is to intimidate the participant. Brusque questions include, “Do you have any weapons on
                you?” and “Do you have anything that can be used as a weapon?” Answers to the second question have
                ranged from “This glass in my hand!” to “My mind!”</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/600/716/1*klaOB28cpkZ3J8a11K_9bQ.png"
                    width="600"><label class="margin-toggle" for="2039562802105925916">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="2039562802105925916"><span class="marginnote">Through the
                    Look Glass. Photo Mikhail Grebenshchikov</span></figure>
            <p>This opening experience is designed to engage your cognitive behaviour and imagination and to be gently
                provocative. Having been primed, you are then directed to enter into the <em>RIOT</em> installation
                quadrant. Then, as you’re watching the film in the installation, the AI watches you back through the
                webcam. The set design by <a href="https://www.kinicho.com/">Kinicho</a> surrounds you in visceral 3D
                sound, debris from the aftermath of a violent disorder is strewn around and rubbish spills from an
                overturned dustbin. A veil of haze and the scent of smoke completes the ambience.</p>
            <p>The <em>RIOT 2</em> prototype has a Machine Learning algorithm that monitors you for the emotions of
                calm, anger, and fear. There are currently four levels: You come into contact with a riot cop, a looter,
                an activist, and a young woman being detained by the police. Your objective is to stay calm, make it
                home alive, and bring the community together.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/1067/1*d6SgvxnSaDDZAPUIYD8vgw.png"
                    width="800"><label class="margin-toggle" for="8385027483886372983">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="8385027483886372983"><span class="marginnote">The RIOT
                    installation and participant</span></figure>
            <p>The response to <em>RIOT 2 </em>(still in development) has been exponential, largely due to the fact that
                it encompasses so many disciplines and technologies: film, gaming, A.I., machine learning, facial
                emotional detection, art, behavioural psychology, neuroscience, epigenetics, the parkour philosophy of
                moving through fear, and racial tension at this current time of intense global social unrest.</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F273625825%3Fapp_id%3D122963&amp;dntp=1&amp;url=https%3A%2F%2Fvimeo.com%2F273625825&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=vimeo"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="904355302836155374">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="904355302836155374"><span class="marginnote">See for yourself how people
                    responded to this latest iteration.</span></figure>
            <p>The <em>RIOT</em> “reality construct” seems to be making an impact in your time:</p>
            <ul>
                <li>The <em>RIOT</em> prototype was honoured as part of the <a
                        href="http://digitaldozenawards.com/">Digital Dozen</a><strong> </strong>Breakthroughs in
                    Storytelling 2017, in which Columbia University School of the Arts’ Digital Storytelling Lab
                    acknowledged the most innovative approaches to narrative from the past year.</li>
                <li>Articles about it have appeared in<a
                        href="https://www.forbes.com/sites/katmustatea/2017/12/30/this-filmmaker-from-the-future-can-teach-you-to-channel-your-fear/">
                    </a><em><a
                            href="https://www.forbes.com/sites/katmustatea/2017/12/30/this-filmmaker-from-the-future-can-teach-you-to-channel-your-fear/">Forbes</a></em>,
                    <a href="http://wp.me/pZawQ-5QBN">CBS</a>,<a
                        href="https://www.fastcompany.com/40498786/when-your-fear-is-the-remote-control"> </a><em><a
                            href="https://www.fastcompany.com/40498786/when-your-fear-is-the-remote-control">Fast
                            Company</a></em>,<a
                        href="https://www.engadget.com/2017/10/13/riot-2-interactive-film-karen-palmer-interview/">
                        Engadget</a>,<a
                        href="http://www.nbcnews.com/tech/security/facial-recognition-technology-raises-privacy-concerns-n676836">
                        NBC</a> and <em>The</em><a
                        href="https://www.theguardian.com/science/blog/2017/mar/29/its-a-riot-the-stressful-ai-simulation-built-to-understand-your-emotions">
                        Guardian</a><em>, </em>to name a few.
                </li>
                <li>I have exhibited <em>RIOT</em> at the prestigious Future of Storytelling Festival in NYC, at the
                    V&amp;A in London, and at PHI Centre’s<a href="https://phi-centre.com/en/event/lucid-realities-en/">
                        Sensory Stories Exhibition</a> in Montreal.</li>
                <li>I was final keynote speaker at<a
                        href="https://zkm.de/en/event/2018/04/encoding-cultures-living-amongst-intelligent-machines">
                        ZKM</a> The AI Conference: Living amongst Intelligent Machines” in Germany and at <a
                        href="http://www.silbersalz-festival.com/de">Silbersalz</a> “Future, Media and Science” in Halle
                    Germany too.</li>
                <li>I’ve spoken at the FoST Global<a href="https://futureofstorytelling.org/summit"> Summit</a> and the
                    <a href="https://arts.columbia.edu/overview-digital-storytelling-strategy">Strategic Storytelling
                        Seminar</a> NY, the Google Cultural Institute<a
                        href="https://www.google.com/culturalinstitute/thelab/"> Lab</a> in Paris, <a
                        href="https://cphdox.dk/en/">CPH:DOX</a>, Copenhagen International Documentary Film Festival,
                    and <a
                        href="https://cphdox.dk/en/themes-and-key-international-speakers-at-cphconference-announced/">Art,
                        Technology &amp; Change</a>.
                </li>
            </ul>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/533/1*K_R2MpVDWluRVXtjGg8MxQ.jpeg"
                    width="800"><label class="margin-toggle" for="6350516808615028727">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="6350516808615028727"><span class="marginnote">Speaking at
                    the communication ritual you know as a “TEDx talk” at the Sydney Opera House</span></figure>
            <p>In September 2017, while continuing to develop the final <em>RIOT</em> iteration, I was accepted as a<a
                    href="https://blog.ted.com/meet-the-fall-2017-class-of-ted-residents/"> TED resident</a> in New
                York. The TED Network functions like a community of underground resistance experts. While there, I
                expounded on the behavioural psychology and neuroscience research. Having spoken to many VR
                practitioners and curators, it appeared that the neurological impact of VR research came several years
                after the initial experiences. Therefore, I felt it essential to build the technology and the story
                alongside the neurological research into AI, in order to be fully aware of the technologies’
                implications and to create an intentional empowering experience for participants.</p>
            <p>I have recently completed a tenure as an<a
                    href="https://thoughtworksarts.io/blog/karen-palmer-ai-residency/"> AI Artist in Residence</a> at
                the ThoughtWorks technology company, where I was continuing to develop the <em>RIOT</em> technology and
                UX Experience.</p>
            <p>You may be aware of ThoughtWorks as a global technology company that pioneered the Agile process. They
                have 5,000 employees in 14 countries. Being from the future, I recognise the ThoughtWorkers as a race of
                consciously evolved beings who reside on their current planet of choice, Earth. They divide their days
                harnessing the most powerful tool in your current culture, aka “technology,” and spend their evenings
                and weekends dedicated to social concerns of the planet.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/600/1*ctH7G2SgQiw6LGvj35wi5g.jpeg"
                    width="800"><label class="margin-toggle" for="5760951550612706260">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="5760951550612706260"><span
                    class="marginnote">ThoughtWorkers channeling higher consciousness.</span></figure>
            <p>The basis of the residency at ThoughtWorks was to develop the <em>RIOT</em> AI system and explore the
                potential for a multi-modality user experience for the final iteration. This meant building EmoPy, an
                open-source library for facial expression recognition that contains neural nets used to measure emotions
                during RIOT.</p>
            <p>Watch the following two videos to learn more about my collaboration with the ThoughtWorkers. This first
                video invited volunteers to be part of the process to collate their expressions as the basis of the
                Neural Net:</p>
            <p>The second video explains the process of how we built a Neural Net:</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fplayer.vimeo.com%2Fvideo%2F275748144%3Fapp_id%3D122963&amp;dntp=1&amp;url=https%3A%2F%2Fvimeo.com%2F275748144&amp;image=https%3A%2F%2Fi.vimeocdn.com%2Fvideo%2F708106203_1280.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=vimeo"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="14678903298272469977">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="14678903298272469977"><span class="marginnote">How to build a Data Set!</span>
            </figure>
            <p>It was important to cultivate a thriving dialogue through a series of Artist Open Studio Sessions with an
                international community of artists, creatives, technologists, neuroscientists, policy advisors, festival
                curators, and academics. (Many of these conscious creators go on to become part of the media resistance
                movement in the future). This was all part of the <em>RIOT </em>development process to converge. 3D
                sound designer Garry Haywood wrote an exquisite article on the types of thoughts coming from these
                events, <a
                    href="https://suppo.site/inside-out-emotion-witness-and-self-awareness-from-immersive-experiences-2ec8d1001a9f">Inside
                    Out: Emotion, Witness and Self-awareness from Immersive Experiences</a>.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/609/1*B1LIMz5IiPgkhbUkIUmBPA.jpeg"
                    width="800"><label class="margin-toggle" for="5457403124117009192">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="5457403124117009192"><span class="marginnote">Artist Open
                    Studio Sessions/ Global Interdisciplinary meetup for conscious creators</span></figure>
            <p>We also carried out extensive research in conjunction with my TED Residency into behavioural psychology
                in order to explore if there is a methodology to shift perception. This research was done in
                collaboration with Neuroscientist Elizabeth Waters and Emily Balcetis, a<a
                    href="https://www.ted.com/talks/emily_balcetis_why_some_people_find_exercise_harder_than_others">
                    TED Speaker</a> and behavioural psychology professor at NYU, (both known cosmic time travelers).</p>
            <p>Emily’s lab, <a href="https://sites.google.com/a/nyu.edu/nyu-spam-lab/home">SPAM</a> (which stands for
                Social Perception and Motivation), focuses on the “conscious and non-conscious” ways that people
                fundamentally orient to the world — how the motivations, emotions, needs, and goals people hold affect
                the basic ways they perceive, interpret, and ultimately react to the information around them. The lab’s
                work explores motivational biases in visual and social perception and the consequential effects for
                behaviour and navigation of the social world. In doing so, the lab’s research represents an intersection
                among social psychology, judgment and decision-making, social cognition, and perception.</p>
            <p>Why is it important to guide your personal sensors (also referred to as your eyes and ears)? Behavioural
                research shows that the way viewers watch dash-cam footage of police brutality reflects their personal
                bias. If the viewer is empathetic towards the police, their eye-tracking follows the trajectory of the
                police. If the viewer is sympathetic to the person being brutalised, their eye tracking follows the
                trajectory of the person they perceive as the victim.</p>
            <p>So I asked the SPAM Lab: “If we were able to encourage a participant to deviate from their predetermined
                eye-tracking through the storytelling experience, would we be able to change their bias or perspective?”
                They concluded this may be possible, and this is what we are now exploring. Often, people are looking at
                only one part of the screen. Through <em>RIOT</em>, the aim is to direct people’s gaze so they see the
                whole story.</p>
            <p>I am experimenting with shifting participants’ perspective through having them access multiple narratives
                from different characters’ perspectives. This will inform the next stage of my R&amp;D into racial
                perspective disparity in America at this particularly turbulent time of racial divide.</p>
            <p>The reality of the disparity of the black experience by white and black people is more evident now than
                ever, as white people attempt to use logic to make sense of the consistent violent incidents against
                young black men. These consist of #SittingWhileBlack at Starbucks, #NappingWhileBlack at Yale, or
                #CookingoutwhileBlack in the park, all offenses that have caused white folks to call police. A shift in
                perception is required to tackle this form of racism, now more than ever.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/600/1*R5FCt33ohLCILBljdnaS7A.jpeg"
                    width="800"><label class="margin-toggle" for="8481352683738284626">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="8481352683738284626"><span
                    class="marginnote">ThoughtWorkers creating a Machine Learning Algorithm undetected.</span></figure>
            <p>As part of the <a
                    href="https://www.thoughtworks.com/insights/blog/how-artificial-intelligence-transforming-criminal-justice-system">research
                    done into AI at ThoughtWorks</a>, we have come to the conclusion that all AI is biased, because it
                is trained on data generated or labeled by humans, who are inherently biased. So we asked ourselves, as
                part of the objective to shift perception: Do we want to intentionally load the machine-learning
                algorithm with bias? This could allow participants to experience <em>RIOT</em> from the perspective of
                being a white woman or black male to see how their experiences would differ.</p>
            <p>This will be explored further in the next phase of development. Your future! My past!</p>
            <p>In the next and final phase of <em>RIOT</em> we’ll be introducing what’s called “multi-modalities,” i.e.
                further functionality to measure more sensors. For example, as part of my ThoughtWorks residency we
                researched voice emotional recognition, which would allow the participant to speak to the film and have
                the narrative branch in different directions depending on the emotion detected.</p>
            <p>I deliberately did not use wearable technology because I want people to step into the “world” of the
                experience and feel that they are affecting the world in the same way when they step outside. This
                experience reveals the larger truth of the holographic universe that we’ve come to understand in the
                future — that we’re all inhabiting a high-res simulation that our brains translate back into what we
                experience as reality.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/533/1*YMbKF2iWgkJvLYCwwcRX6Q.jpeg"
                    width="800"><label class="margin-toggle" for="2203657836066435576">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="2203657836066435576"><span class="marginnote">RIOT
                    participant inside the reality construct installation @ FoST</span></figure>
            <p>I conclude this transmission with an invitation: We are seeking more partners in the Resistance!</p>
            <p>As your world moves closer to my future, <em>RIOT</em> will continue to evolve as a tool to help humans
                better apprehend and control their relationship with provocative stimuli and coercive media. As part of
                the next phase of storytelling development, I will be interviewing people that have been in riots to
                develop a documentary-based script to produce the new <em>RIOT</em> film. This will be done in
                collaboration with the National Theatre Immersive Storytelling Studio and its in-house screenwriter.</p>
            <p>In addition, we are currently seeking partners/funders/resources to complete the final iteration of this
                sensory story experience. To contact us please resonate your energy towards our frequency, or use more
                conventional modes of communication.</p>
            <p>I close this transmission as I started, making you aware of the impending revolution of technology, so
                you are prepared and enabled. As a tool in your struggle, we leave you a gift: We are releasing to you,
                the people, a derivation of our own Emo Py A.I. Neural Net. The system has <a
                    href="https://github.com/thoughtworksarts/EmoPy">been made open source</a> in order to provide free
                access beyond existing closed-box commercial implementations, both widening access and encouraging
                debate. Use it well!</p>
            <p>Many of you are already part of this invisible movement, so exclusive that you do not even know you are
                in it. We know you are out there and waiting. This is a beacon in the darkness in this storytelling
                revolution with art and technology.</p>
            <p>Hear this call to join us! Your future needs you! The time has come!</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FNIpeUdKK2-4%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DNIpeUdKK2-4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FNIpeUdKK2-4%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe></figure>
            <p><em>[End transmission]</em></p>
            <h2 id="fc2c">Read the response</h2>
            <p><a
                    href="https://immerse.news/from-the-storyteller-from-the-present-to-the-storyteller-from-the-future-8d2fa9c2ebe9">From
                    the Storyteller from the Present to the Storyteller from the Future
                </a>by Eliza Capai</p>
            <blockquote>
                <p><em>Immerse</em> is an initiative of the MIT Open DocLab and The Fledgling Fund, and is fiscally
                    sponsored by IFP. Learn more about our vision for the project <a
                        href="https://immerse.news/whats-our-editorial-vision-82d7eeb3e7b9#.2vsd5nxxm">here</a>.</p>
            </blockquote>
        </section>
    </article>
</body>

</html>