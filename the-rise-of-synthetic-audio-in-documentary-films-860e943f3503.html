<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>The Rise of Synthetic Audio in Documentary Films</title>
    <link href="css/app.css?id=a32ac4a32f2afc26c125" rel="stylesheet" media="screen" data-turbolinks-track="reload">
    <script src="/js/app.js?id=86651c0aa91db5eb001b" defer="true" data-turbolinks-track="reload"></script>
    <meta name="turbolinks-cache-control" content="no-cache">
    <meta name="csrf-param" content="_csrf">
    <meta name="csrf-token" content="lPH1NuFuIM13a5bt0MLDCSi9D64OnyjcZMjfIVuJ7-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <h1>The Rise of Synthetic Audio in Documentary Films</h1>
    <p class="meta"><a href="https://medium.com/u/bfb4d038640c">Dan Schindel</a> on 2022-04-07</p>
    <article>
        <section>
            <h3 id="f28c">Examining the use of voice models from “Val” to “The Andy Warhol Diaries”</h3>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/536/1*_-0Otafvp5-s_j7MSa8n2A.jpeg"
                    width="800"><label class="margin-toggle" for="6590691746898168906">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="6590691746898168906"><span class="marginnote">Still image
                    from <em><strong>The Andy Warhol Diaries</strong></em></span></figure>
            <p>In the summer of 2021, mass audiences got an abrupt introduction to audio deepfaking thanks to the <a
                    href="https://www.newyorker.com/culture/annals-of-gastronomy/the-ethics-of-a-deepfake-anthony-bourdain-voice">controversy
                    over the documentary </a><em><a
                        href="https://www.newyorker.com/culture/annals-of-gastronomy/the-ethics-of-a-deepfake-anthony-bourdain-voice">Roadrunner:
                        A Film About Anthony Bourdain</a></em><em> </em>(2021)<em>. </em>To provide narration for
                several lines from deceased subject Anthony Bourdain’s private writings, director Morgan Neville and his
                crew used AI software to generate a digital facsimile of Bourdain’s voice. (The software employed has
                not been disclosed as of this date.) By doing this without the permission of Bourdain’s widow, nor any
                disclosure of its implementation, <em>Roadrunner </em>aroused conversation around the ethics and
                aesthetics of this technology. Such conversations will only continue as more media incorporating
                artificially-generated audio continues to come out. Several other recent documentary projects made
                similar use of synthetic voices; while they didn’t raise as many eyebrows as <em>Roadrunner,</em> they
                deserve just as much scrutiny.</p>
            <h2 id="5378">From actors to “voice models”</h2>
            <p><em>Val </em>(2021) is<em> </em>a biography of actor Val Kilmer, known for his roles in <em>Top Gun,
                    Batman Forever, Heat, </em>and much more.<em> </em>Since Kilmer’s voice had been drastically altered
                by a recent battle with throat cancer, the film took a traditional documentary workaround to give voice
                to his thoughts, employing a soundalike to read his words as narration. For an added bit of poignancy,
                <a
                    href="https://www.latimes.com/entertainment-arts/movies/story/2021-07-23/val-kilmer-children-documentary-amazon-jack-mercedes">that
                    soundalike was Kilmer’s son, Jack</a>, who sounds fairly close to the way Kilmer did in his <em>Top
                    Gun </em>days.
            </p>
            <p>Not long after production wrapped, the crew reached out to British AI company <a
                    href="https://www.sonantic.io/">Sonantic</a> to <a
                    href="https://www.sonantic.io/blog/helping-actor-val-kilmer-reclaim-his-voice">create a “voice
                    model”</a> for Kilmer, which would help him communicate going forward. The model is similar in
                principle to preexisting speech-generating devices, but rather than produce a robotic or generic voice
                from the input text, the speech will instead sound like Kilmer’s. While Kilmer has been dubbed by other
                actors for roles taken since his tracheotomy, audiences will be hearing <a
                    href="https://www.youtube.com/watch?v=OSMue60Gg6s">the model at work</a> in November’s <em>Top Gun:
                    Maverick</em> (2022)<em>.</em> This case reflects the more optimistic potential for this technology,
                as a natural evolution of speech generators used by disabled people for decades that facilitates
                communication in voices that better reflect their users.</p>
            <p>In most cases, however, the implementation of audio deepfaking in film receives mixed-to-negative
                responses. The most high-profile example yet came with the Disney+ <em>Star Wars </em>series <em>The
                    Mandalorian</em> (2019)<em> </em>and <em>The Book of Boba Fett </em>(2021)<em>. </em>The character
                of Luke Skywalker, originally portrayed by Mark Hamill in the films, appears entirely via a mixture of
                different computer-generation tools. A de-aged rendering of Hamill’s face is placed on the body of a
                double, while Luke’s voice is created entirely through AI software. That audio <a
                    href="https://www.respeecher.com/case-studies/respeecher-synthesized-younger-luke-skywalkers-voice-disneys-mandalorian?hsLang=en">was
                    created by Respeecher</a>, which has recently also been utilized to resurrect the likes of <a
                    href="https://www.respeecher.com/case-studies/revealed-respeecher-creating-digital-vince-lombardi-super-bowl-lv?hsLang=en">Vince
                    Lombardi</a> and <a
                    href="https://www.respeecher.com/case-studies/manuel-rivera-morales-voice-re-created-ai-olympic-games?hsLang=en">Manuel
                    Rivera Morales</a> for sports promotions. Many commentators reacted poorly to the deepfaked Luke,
                with <a
                    href="https://boingboing.net/2022/02/10/how-an-ai-neural-network-brought-luke-skywalkers-voice-to-the-book-of-boba-fett.html">BoingBoing
                    pointing out</a> that electing to use an algorithm over Hamill’s own voice is especially odd, given
                that he is a prolific voice actor<strong> </strong>(and, of course, still alive).</p>
            <p>It’s fascinating that Respeecher’s catalog of success stories includes Luke Skywalker alongside deceased
                sports figures, whose appearances in commercials evoke the recurring trend of using CG recreations of
                celebrities like <a href="https://apnews.com/article/253758edc00908fc9f65d30cd7e6c4b2">Fred Astaire</a>
                and <a
                    href="https://www.theverge.com/2013/2/28/4039144/audrey-hepburn-resurrected-for-galaxy-chocolate-commercial">Audrey
                    Hepburn</a> in advertising. AI techniques seem to have brought this fad back into vogue. Since our
                culture trades heavily in iconic figures and nostalgia, it makes sense that so many resources are
                invested in trying to better imitate these icons. (And in the case of Disney, since the icons are
                lucratively trademarked, it’s in their interest to find ways to preserve them past the deaths of their
                original portrayers.)</p>
            <h2 id="7806">Case study: <em>The Andy Warhol Diaries</em></h2>
            <p>This March saw the release of <em>The Andy Warhol Diaries </em>(2022)<em>, </em>a Netflix docuseries
                narrated by a Warhol voice generated by <a href="https://www.resemble.ai/">Resemble AI</a>, which read
                excerpts from the pop artist’s voluminous memoirs. (This production received the permission of the
                Warhol estate to use the tech, as multiple disclaimers inform the viewer.) In press notes, director
                Andrew Rossi explains that he wanted to “communicate that experience of having such an intimate
                connection to Andy, without having an actor in the middle mediating that relationship.” He also believes
                this move to be in keeping with Warhol’s philosophy, citing the artist’s <a
                    href="https://www.nationalgalleries.org/art-and-artists/features/andy-warhol-and-eduardo-paolozzi-i-want-be-machine">expressed
                    desire to “be a machine”</a> and his lifelong interest in creating avatars and alternate personae in
                his work.</p>
            <p>We can’t know for sure what Warhol would think of the series’ “AI Andy,” but Rossi’s justification is
                interesting for a different reason — the implicit claim that this use of AI is somehow not mediating the
                relationship between Warhol and the audience. In a statement to <em>Immerse</em>, Resemble AI founder
                and CEO Zohaib Ahmed said, “The creative team wanted to add as much authenticity to the docuseries [as
                possible], and thought that an AI representation of Andy Warhol would create a compelling and immersive
                experience.” The assumption is that the viewer more readily accepts this voice than they would a
                soundalike or some other stand-in — the same logic that leads to creating an all-AI Luke Skywalker
                rather than simply recasting him.</p>
            <p>Yet this <a href="https://www.resemble.ai/andy-warhol/">lengthy process</a> is, if anything,
                <em>more</em> mediated. It consisted of a great deal of tinkering with the algorithms, tones and pitch,
                and innumerable other tiny details. It was all built on a mere three minutes’ worth of usable audio.
                Additionally, it did involve a performer: character actor Bill Irwin read Warhol’s lines as a reference
                for the technicians. In an email back-and-forth with <em>Immerse</em>, a press representative confirmed
                that Irwin read “all the lines in Andrew [Rossi]’s script.” The result, then, is something like an
                audial version of an Andy-Serkis-esque motion capture performance, with the digital tools’ result
                informed heavily by human intervention.
            </p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FS5hQHSkTm1U%3Ffeature%3Doembed&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DS5hQHSkTm1U&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FS5hQHSkTm1U%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe></figure>
            <p>And even with the nuances added by the sound engineers and Irwin’s guidance, the AI Andy has the same
                issue as Skywalker and many other deepfaked voices. It echoes from the uncanny valley, full of flat
                pronunciations and odd waverings that undermine the attempt at realism. Though futurist boosters
                forecast that soon these kinks will be ironed out, the fact that Resemble AI needed to bring in Irwin
                suggests it could be harder for the tech to generate convincing voices than they claim. It also makes
                one wonder how much of deepfaking may be <a href="https://reallifemag.com/potemkin-ai/">a Potemkin
                    situation</a>. This is difficult to gauge, of course, since much of the process remains opaque to
                outsiders.</p>
            <p>Ahmed told <em>Immerse</em> that some of the most frequent purchases of his service were for the sake of
                “Saving voice actors the time and expense of traveling to studios,” or “Help[ing] customer service teams
                … address the need for language translation quickly and cost-effectively,” or “Keeping up with new
                gaming releases, so the audio stays relevant.” Despite the artistic possibilities here, it seems that as
                usual, industry demand for shortcuts is doing more to drive this technology’s development than anything
                else. For documentary filmmakers, that leaves a lot more experimentation and discovery to be done.</p>
            <p><em>For more news, discourse, and resources on immersive and emerging forms of nonfiction media, </em><a
                    href="https://immerse.news/subscribe-to-our-monthly-newsletter-aa39eb11ccc2">sign up</a><em> for our
                    monthly newsletter.</em></p>
            <p>Immerse<em> is an initiative of the MIT Open DocLab and Dot Connector Studio, and receives funding from
                    Just Films | Ford Foundation, the MacArthur Foundation, and the National Endowment for the Arts. The
                    Gotham Film &amp; Media Institute is our fiscal sponsor. Learn more </em><a
                    href="https://immerse.news/whats-our-editorial-vision-82d7eeb3e7b9">here</a>. We are committed to
                exploring and showcasing emerging nonfiction projects that push the boundaries of media and tackle
                issues of social justice — and rely on friends like you to sustain ourselves and grow. <a
                    href="https://fiscal.thegotham.org//project.cfm/1074/">Join us by making a gift today</a><em>.</em>
            </p>
        </section>
    </article>
</body>

</html>