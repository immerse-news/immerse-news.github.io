<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Field report: What’s New With VR in the Newsroom?</title>
    <link href="css/app.css?id=a32ac4a32f2afc26c125" rel="stylesheet" media="screen" data-turbolinks-track="reload">
    <script src="/js/app.js?id=86651c0aa91db5eb001b" defer="true" data-turbolinks-track="reload"></script>
    <meta name="turbolinks-cache-control" content="no-cache">
    <meta name="csrf-param" content="_csrf">
    <meta name="csrf-token" content="X0n6S8N7GA1mRk_Hd4oLjKM-XJGxSgwxVbTZMjrW3S0">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <h1>Field report: What’s New With VR in the Newsroom?</h1>
    <p class="meta"><a href="https://medium.com/u/51d22d432344">Jessica Clark</a> on 2017-12-28</p>
    <article>
        <section>
            <h2 id="8ec0">Field Notes: What’s New With VR in the Newsroom?</h2>
            <p>“In the new year, people will get real about virtual reality,” <a
                    href="http://www.niemanlab.org/2017/12/vr-reaches-the-next-level/">suggests</a> Ray Soto, Gannett’s
                director of emerging technologies, in the Nieman Lab’s <a
                    href="http://www.niemanlab.org/collection/predictions-2018/">annual round-up</a> of predictions for
                journalism.</p>
            <p>But what does “getting real” entail? Soto was one of four VR innovators I interviewed at a pair of
                conferences this fall—the <a href="https://ona17.journalists.org/sessions/vrfilmfest/">Journalism 360
                    Immersive Storytelling Festival</a> (which took place at the Online News Association’s <a
                    href="https://ona17.journalists.org/">annual gathering</a>), and the <a
                    href="http://doubleexposurefestival.com/">Double Exposure Investigative Film Festival</a>. See our
                conversations below to learn how reporters and technologists are putting VR to work.</p>
            <h3 id="e886">Laura Hertzfeld, Director, Journalism 360</h3>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/200/200/1*9oRhtfA3WmMIWIrd35JPXA.png"
                    width="200"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p><em><strong>Tell us about the Journalism 360 project.</strong></em></p>
            <p><a href="https://medium.com/journalism360">Journalism 360</a> is funded by the Knight Foundation and
                Google News Lab and run through ONA. We funded a bunch of projects through our <a
                    href="https://journalists.org/2017/07/11/journalism-360-challenge-awards-285000-11-projects-advance-use-immersive-storytelling-news/">grant
                    challenge</a>, which started in March of 2017, and announced the winners in July. Eight of the 11
                grantees are here.</p>
            <p>The projects ranged from an augmented reality project on facial recognition that identifies bias when
                you’re reading news stories, to room-scale VR experiences like the one from <em>USA Today</em> on the US
                border wall. So there’s a huge range in how we’re defining immersive storytelling. Our bigger goal is to
                build a community around immersive storytellers.</p>
            <p><em><strong>You’ve been editing your Medium blog for about a year. What are some of the surprising things
                        these projects are doing?</strong></em></p>
            <p>I like that we’re defining “immersive” very, very broadly. There are some things shot on <a
                    href="https://www.insta360.com/product/insta360-nano/">Insta360 Nanos</a> that get a little
                narration and graphics. <em>Euronews</em> is doing <a
                    href="http://www.euronews.com/2017/03/17/svalbard-seed-vault-banking-on-the-future">a project</a> on
                the Svalbard seed vault, about the future of the environment. It’s all 360 video.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/452/1*4ChKz-nXbv7n7bkiUK6_jg.png"
                    width="800"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p>Then you have the really crazy tech stuff that <a
                    href="http://emblematicgroup.com/experiences/solitary-confinement/">Emblematic and FRONTLINE</a> are
                doing with avatars of humans, which brings up a lot of ethical questions. We have Robert Hernandez from
                USC training the next generation of journalists; USC is one of only a handful of programs doing this
                work…</p>
            <p><em><strong>What are the big questions journalists have when they want to start doing this?</strong></em>
            </p>
            <p>We have a lot of “how to” info on our Medium blog. What cameras should I use? How much does it cost?</p>
            <p>On the panel yesterday, Raney Aronson from FRONTLINE said there isn’t really a price-point yet for these
                big projects because we just haven’t done enough of them. A lot of companies that work with Hollywood
                and with gaming studios are interested in working with news organizations because they want to do this
                stuff. It’s still very expensive to do big projects but relatively cheap to go out with a 360 camera and
                do a trial-and-error experiment.</p>
            <p><em><strong>Anything else Immerse readers should know?</strong></em></p>
            <p>I’m probably more interested in hearing from them! We do monthly Google Hangouts on postproduction, gear,
                and projects we think are cool. I’m really interested in film festivals and the overlap between the art
                and documentary world.</p>
            <h3 id="2a6b">Ray Soto, Director of Emerging Technology, Gannett</h3>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/200/200/1*j00lBwEti1anWplsNlVNHw.png"
                    width="200"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p><em><strong>Tell us about </strong></em>USA Today’s<em> </em><a
                    href="https://www.usatoday.com/border-wall/usa-today-network-border-project-about-vr-podcasts-map/">multimedia
                    production</a><em><strong> about the border wall.</strong></em></p>
            <p>It’s a network feature story we worked on for over 3 months with several colleagues from the <em>Arizona
                    Republic</em> and other news outlets near the border.</p>
            <p>Our goal, particularly with the VR side, was to create an experience that relates to different topics
                along the border: human smuggling, drug trafficking, impacts on the communities and the environment.</p>
            <p>We wanted to leverage LIDAR and photogrammetry to recreate the terrain, to create on-the-ground
                experiences that really help folks feel that sense of presence. We pulled lots of technologies into this
                and what we ended up learning fairly quickly is that the production process was more complex that we
                expected. But once we had our environment, we realized we had something pretty special and were on the
                right track.</p>
            <p><em><strong>I know about </strong></em><a
                    href="http://www.photogrammetry.com/">photogrammetry</a><em><strong> but what is
                        LIDAR?</strong></em></p>
            <p>LIDAR is light radar detection. We partnered with a company called <a
                    href="http://www.aerialfilmworks.com/">Aerial Filmworks</a>, which had LIDAR catcher technology
                within a helicopter. Think about a laser ping capturing the topography. LIDAR creates a <a
                    href="https://knowledge.autodesk.com/support/autocad-map-3d/learn-explore/caas/CloudHelp/cloudhelp/2016/ENU/MAP3D-Use/files/GUID-7C7DD8A7-B561-45B0-A803-852E0A667F3C-htm.html">point
                    cloud</a> system that can help you visualize the undulation of the terrain. But LIDAR wasn’t
                necessarily giving us what we needed for a VR on-the-ground experience. When you take that point cloud
                system, you render it as a 3D object, and there was a lot of manual work needed to make it look good. So
                we ended up relying more on photogrammetry; that worked out much better than LIDAR.</p>
            <p><em><strong>How do you do photogrammetry on a giant mountain?</strong></em></p>
            <p>The process was fairly straight-forward. I’ll use Big Bend as an example: the portion that we had
                rendered out was about 1200 photographs at around 100 megapixels each. That’s 16.8 gigabytes worth of
                data. Within each image, the metadata for geolocation was incorporated. So you pull all of those into a
                program, set it out, and it shows you the flight path and renders out a 3D object. That’s essentially
                the process, but then there’s optimization with 3D software: getting it to render and look halfway
                decent.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/452/1*5sLaVsL0CARZUy2W0u-PGQ.png"
                    width="800"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p><em><strong>So I’m the user and I’m looking at something like a 3D Google map. I’m looking from above and
                        see an outline of the proposed border wall and then I zoom into the landscape to get to the
                        stories. Is that how it works?</strong></em></p>
            <p>We were working with so many different journalists we wanted to be able to pull in as much content as we
                could in order to be able to have a narrative arc across these different locations along the border —
                ultimately, across four different chapters. So when you first get in we set the tone with strong
                narrative. Explore, see for<em> </em>yourself, and learn. Our goal was to educate.</p>
            <p><em><strong>I see people using the Vive to walk around the animation. Can they also use Google
                        Cardboard?</strong></em></p>
            <p>For this particular project, not yet. We build this exclusively for the Vive to have that room-scale
                experience. It doesn’t perform very well on mobile devices.</p>
            <p><em><strong>This sounds like a big, expensive project. What is the projected life span?</strong></em></p>
            <p>We’re thinking about this as an evergreen project. We identified stories that we feel are strong enough
                to stand on their own for a while. Those stories won’t change: the impact on the environment, the drug
                smuggling aspects. If there is an opportunity, we’ll revisit it. But the production process took about
                three months. Two months was just exploring how to build the terrain, LIDAR, and photogrammetry. Once we
                figured out that process we built this within two weeks.</p>
            <p><em><strong>So what do you think: Should they build the wall?</strong></em></p>
            <p>It’s not for me to say. What we were hoping to do with this project is help folks understand that this is
                a more complex issue than whether a wall should be built or not. There are already some fences and walls
                built along the path but not as many as you may think. There are three different terrain types, which
                really complicates things. Big Ben is essentially a massive canyon: You’ll never be able to build a wall
                there… Anyway, I’d encourage audiences to come by and <a
                    href="https://www.usatoday.com/border-wall/">experience this work</a> for themselves. We’d love
                feedback.</p>
            <h3 id="1ca8"><a href="http://benkreimer.com/">Ben Kreimer,</a> independent journalism technologist</h3>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/227/300/1*WvoqVQUKM4_zzgI2ZRZL-w.png"
                    width="227"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p><em><strong>So, tell me about the project I just experienced.</strong></em></p>
            <p>You saw <em>Making Waves: Women and Sea Power</em> on the HTC Vive. It’s the story of a group of women
                seaweed farmers in Zanzibar who are innovating in the way they are growing seaweed, planting in deeper
                waters. Due to sea temperature rising they can no longer grow seaweed in shallow waters; seaweed farming
                is their primary source of income.</p>
            <p><em><strong>So what were you hoping to do by filming these women in 360 video?</strong></em></p>
            <p>Take you to Zanzibar to see the places where they work and live, to see the work that they are doing
                underwater.</p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FYfk7MJh5jA4%3Ffeature%3Doembed&amp;url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYfk7MJh5jA4&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FYfk7MJh5jA4%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe></figure>
            <p><em><strong>You had to film on land and underwater. What were some of your technical
                        challenges?</strong></em></p>
            <p>Underwater brings up a whole host of challenges. Cameras fog up. You’ve got refraction, and so your lens
                field-of-view is less than it is on land. We used a 3D-printed mount from <a
                    href="https://www.shapeways.com/">Shapeways</a>, using 3 Kodak SP360 4K cameras. On land, shooting
                is pretty straight-forward. For the aerial shots I have a custom <a
                    href="http://benkreimer.com/home/mavic-360-video-drone-cropped/">DJI Mavic Pro 360</a>.</p>
            <p><em><strong>When I was on land, I couldn’t see the camera. But you had a diver with a camera next to me
                        underwater. Is it harder to “erase” the camera underwater? Or was there another reason why you
                        left the diver in?</strong></em></p>
            <p>For the underwater shots, we wanted to leave the diver in because the story was partly about the
                relationship between the humans and the ocean. It was part of the shot. We never wanted or even
                considered removing the diver, verses the other shots, with just the tripod on land.</p>
            <p><em><strong>The moment that struck me most was when I was surrounded by the women laughing. What was your
                        favorite moment?</strong></em></p>
            <p>I’d probably go with the drone shots. I’ve been working with drones for about 5 years. When I was out
                flying over the seaweed, it was really striking. That aerial perspective is always a little bit magical.
                But I also like the scuba diver going through the small cave, with the cloud of fish. THAT is a 360
                shot. That is what 360 was meant to capture.</p>
            <h3 id="574b">Debra Anderson, co-founder and CEO, Datavized</h3>
            <p><em><strong>Okay, give me your pitch. What is Datavized?</strong></em></p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/200/248/1*4jAf_z3Gkzj_6PNqXZVJMw.png"
                    width="200"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p>Datavized is a New York-based web VR software company. We’re building tools to enable 3D, interactive
                data visualization. We’re focusing on geospatial applications because we found, via prototyping, that
                bringing, say, a 3D bar graph into VR is not in itself compelling. This is a new medium, so we need to
                start from scratch.</p>
            <p>The question we often get, even from VR enthusiasts, is, “Why VR?” An environment with street-level view
                in VR really makes sense. Whether you are using teleporting controllers or a browser in a 2D space, you
                can have full interactivity. What we’re building is hybrid software that enables the view of 3D
                interactive content on the web while also providing capabilities for an immersive experience. If the
                user has a headset, the immersive view is enabled.</p>
            <p>We’re testing around this hypothesis: Is this technology suited for providing an objective view of data
                as well as a subjective experience? We believe it is. Data is meaningless without context. Without
                context, it’s just raw stuff. So we’re building tools that will enable new kinds of mapping.</p>
            <p>We’re taking CSV spreadsheets and mapping data into these environments… We’re building 3D Earth, the
                ability to have country templates and city templates that you can search by zip code. We’re envisioning
                the start of a platform. We see web VR being transformative. You don’t need headsets to access it. We’re
                looking to get tools into the hands of journalists.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/705/501/1*XYgqdt7yOC5usolflUuhPw.png"
                    width="705"><label class="margin-toggle" for="10862164220640835750">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="10862164220640835750"><span class="marginnote"></span>
            </figure>
            <p><em><strong>So, for the uninitiated, “WebVR” means that I can view the work on my browser and interact
                        with it?</strong></em></p>
            <p>Yes, you are viewing a web page. Because it’s VR you have a fully interactive 3D model. If you’re on your
                tablet you might be moving around. If you’re on your phone you might be in magic window mode. And if
                you’re on controllers you may be teleporting in that environment, as if walking through a web page.</p>
            <p><em><strong>What kind of stories can you tell within the platform that you’re building?</strong></em></p>
            <p>The stories span anywhere from climate change to housing to urban planning. We’re providing local
                context, with citizen journalism. We’re integrating a lot of open data as well, so we see these
                experiences as an extension of our digital lives. Texting a VR experience, messaging it, embedding it in
                a blog post….</p>
            <p><em><strong>We were talking before about how you’re kind of building a “data sandwich.” It has different
                        levels, like Google Earth, where people can zoom into different levels of specificity. Could you
                        talk about those levels?</strong></em></p>
            <p>We built a 3D Earth using NASA satellite imagery. We have a prototype where you can click on a country
                and view things like life expectancy or GDP. We’re doing something like this for the UN. You can view
                air pollution by country, and then dive into more detail in the data sandwich. You can go from satellite
                images to terrain data. As you zoom in, if you’ve been in Google Earth VR, it’s a similar context with a
                subjective view of terrain data and mapping political boundaries. When full-throttle in the data
                sandwich, you’re going into city-scale. With city views, you can fly over cities or zoom into that
                environment.</p>
            <p><em><strong>Part of what you’re tapping into is open-source data available online. Are you worried that
                        those sources will dry up at some point?</strong></em></p>
            <p>We want to look at private databases, not just open data. We’re starting with the geospatial data but
                that can evolve. It will take experimentation, research and development. In terms of building the layers
                of earth and city, we’re needing to customize the product. For open data we’re just at the tip of the
                iceberg in developing the product. We are talking with Google News Lab, Enigma and other partners. We
                really want to get these open data products to be useful for search. There’s a plethora of data. That’s
                not the problem. The challenge is giving it context.</p>
            <blockquote>
                <p><em>Immerse </em>is an initiative of Tribeca Film Institute, MIT Open DocLab and The Fledgling
                    Fund<em>. </em>Learn more about our vision for the project <a
                        href="https://immerse.news/whats-our-editorial-vision-82d7eeb3e7b9#.2vsd5nxxm">here</a>.</p>
            </blockquote>
        </section>
    </article>
</body>

</html>