<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>“Prepare, Don’t Panic” for Deepfakes</title>
    <link href="css/app.css?id=a32ac4a32f2afc26c125" rel="stylesheet" media="screen" data-turbolinks-track="reload">
    <script src="/js/app.js?id=86651c0aa91db5eb001b" defer="true" data-turbolinks-track="reload"></script>
    <meta name="turbolinks-cache-control" content="no-cache">
    <meta name="csrf-param" content="_csrf">
    <meta name="csrf-token" content="Wp_B4SC1ZsJFKYnPMKjQf9X0Un_wTyjPxAXVrOPu8n8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
</head>

<body>
    <h1>“Prepare, Don’t Panic” for Deepfakes</h1>
    <p class="meta"><a href="https://medium.com/u/92f81186464c">Srushti Kamat</a> on 2020-11-06</p>
    <article>
        <section>
            <h3 id="8d95">A summary of key insights from Deepfakery, a web series presented by the Co-Creation Studio
                and WITNESS</h3>
            <p>You catch yourself laughing at a retweet of Ben Shapiro’s face on Cardi B and Megan Thee Stallion’s
                bodies in a contemporary rendition of the WAP music video. You know it’s fake. A few posts later, you
                see Mark Zuckerberg talking about Facebook’s complete lack of intention to make the world a better
                place. It looks real but his message seems uncharacteristic. You’re unsure. Both videos are labelled
                deepfakes. But do they serve the same function?</p>
            <p>From August to October this year, the Co-Creation Studio and WITNESS launched a web series, titled
                <em>Deepfakery</em>. The six-episode series featured 20 panelists from areas such as activism, academia,
                archives, art, documentary and engineering all participating in a critical conversation around the
                theme: Prepare [for deepfake technology], Don’t Panic.
            </p>
            <p>In recent years, news reports on the rise of deepfakes have both raised concerns and provided room for
                amusement. The Merriam-Webster dictionary defines a deepfake as <em>a video that has been edited using
                    an algorithm to replace the person in the original video with someone else (especially a public
                    figure) in a way that makes it look authentic.</em></p>
            <p>Although deepfakes are predominantly audiovisual, they fall under an ecosystem of disinformation amid
                shifting understandings of reality. But this ecosystem isn’t new. Alteration of truth has been a
                satirical, artistic and journalistic tool used across mediums for centuries.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/800/450/1*_tgxe9snnCRwebazBg05vA.png"
                    width="800"><label class="margin-toggle" for="8363310397717337897">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="8363310397717337897"><span class="marginnote">The
                    <em>Deepfakery</em> series logo. (<a href="https://cocreationstudio.mit.edu/deepfakery/">Co-Creation
                        Studio</a>)</span></figure>
            <p>As Daniel Howe, an artist behind the project <em>Spectre,</em> says in Episode 1, “Synthetic media in
                various forms has been around almost as long as media itself. Art, satire, and activism have been used
                in a wide range of socially positive ways, mediating between technology and public discourse. So, as
                Walter Benjamin, the cultural critic, said almost 100 years ago, ‘Art is the product of this very
                conversation between technology and society.’”</p>
            <p>Taking another angle, Karen Hao of <em>MIT Technology Review</em> speaks in Episode 2 about how such
                elements of satire are viewed by creative agencies and the advertising industry as a way to infiltrate
                popular discourse. She mentions the need to decide how we label what is considered a deepfake so we can
                then develop the boundaries around its ethical use.</p>
            <p>Regardless of their usage for art and satire, there is no denying that deepfake videos pose unique
                threats. As Jane Lytvynenko, a senior reporter at BuzzFeed News, says in Episode 2<em>, </em>“Videos are
                easily taken out of context and miscaptioned, which is a very big problem for misinformation. And part
                of the reason why they’re a much bigger problem than something like a written article is that people
                very much lean towards the seeing-is-believing gut instinct.”</p>
            <p>While sophisticated AI technology is accessible to few, most deepfakes online originate from
                non-technical users. <a
                    href="https://www.theguardian.com/technology/2020/jan/13/what-are-deepfakes-and-how-can-you-spot-them">In
                    September 2019, the firm Deeptrace found that out of 15,000 deepfake videos, 96% were pornographic.
                    Faces from female celebrities were mapped onto porn stars.</a> Samantha Cole, staff writer at Vice,
                sheds light on the key question of gender dynamics shaping the majority of the videos today. “Deepfake’s
                origin was in making celebrity porn. There was Photoshop with static images and then deepfakes took it a
                step further using algorithms to add them to videos. The people I talk to aren’t machine learning
                experts but rather people interested in using this new technology. But what they miss out on is consent.
                Most of the deepfakes that exist online are porn.” As she states, creators making this porn are not
                engineers but rather people without technical expertise who utilize the tools.</p>
            <p>The nexus of the deepfake conversation today centers on a need for global media literacy. The speed and
                traction with which community-generated deepfake videos spread online has shifted perceptions of truth.
                User discernment on what constitutes as fake is a critical factor in determining collective trust
                towards digital media. However, within the conversation about literacy, there is a tendency to overlook
                regional subjectivities. These regional contexts depend upon the socio-political implications of the
                producers and users, which intrinsically rests upon existing power dynamics.</p>
            <p>In Episode 6 of <em>Deepfakery</em>, Adebayo Okweowo, Africa Program Manager at WITNESS points out this
                need: “Media literacy comes up over and over again because not a lot of people know how to quickly
                identify or spot a shallow fake. And of course, that is very prevalent on the African continent.
                Miscontextualization, sharing stuff that was not relevant to issues, went viral. It should be stopped
                and tracked.” Similarly, Brandi Collins Dexter of the Shorenstein Center sheds light on considerations
                to Black communities in the US when discussing digital engagement. She says, “Black people over a range
                of ages are more likely to engage with new technology and gaming consoles and share new technology and
                content. Also, humor, parody and satire are often part of cultural exchanges.”<em> </em>When attempting
                to mitigate misinformation, we run the risk of overlooking cultural specificities of different groups,
                especially when examining who uses these altered media forms at their inception.</p>
            <p>Furthermore, in Episode 4, Megha Rajagopalan, international correspondent at Buzzfeed News discusses the
                intersection of deepfakes and surveillance. When speaking to the possibility of governments using
                deepfakes to implicate activists or journalists, she asks, “How much more powerful could it be if it was
                possible to completely fabricate conversations particularly in an environment where the media is already
                heavily censored and heavily constrained?” In state-controlled systems, deepfakes could well be used as
                excuses or caveats to further control free speech.</p>
            <figure><img src="https://cdn-images-1.medium.com/fit/c/259/383/1*ZYPmIxDmWfWN-LOesyCoYg.jpeg"
                    width="259"><label class="margin-toggle" for="6788931592706228528">&#9997;&#xFE0E;</label><input
                    class="margin-toggle" type="checkbox" id="6788931592706228528"><span class="marginnote">Official
                    poster for Welcome to Chechnya. (HBO)</span></figure>
            <p>One potential model for intentional and ethical usages of deepfakes comes from David France, journalist,
                filmmaker and director of the HBO film <em>Welcome to Chechnya.</em> To maintain the anonymity of LGBTQ+
                activists in Chechnya, France worked with Ryan Laney, a veteran VFX specialist to find faces that could
                be used to replace the activists via deepfake technology and help protect their identities. France
                sourced LGBTQ+ social media influencers who already had public presences around their sexual or gender
                orientations. Episode 3 of the series explores France’s methodologies, ethics and findings. Despite
                these advances, what might a dialogue with adjacent industries such as production and casting look like
                as documentary filmmakers start using deepfakes? In Episode 6, artist/director Franscesca Panetta
                discusses the implications for actors who may voice altered characters. Panetta brings up the need for
                the actors’ union SAG-AFTRA to update its policies which at present do not cover boundaries and
                regulations for actors taking up AI work.</p>
            <p>What is apparent from the conversations that came out of this series is that a public discourse around
                deepfakes has only just begun. This technology has slowly but powerfully generated a massive trail of
                new considerations around misinformation, truth, and artistic usage. It may well take five to ten years
                for the technology to become available for real-time manipulation. Meanwhile, building an ecosystem
                which prepares societies to protect the most vulnerable while also maintaining freedom of speech will
                require the interrogative work of policy-makers, big tech organizations, scholars, artists and
                journalists working together.</p>
            <p>Watch the episodes below:</p>
            <p>Episode 1 — <em><strong><a
                            href="https://lab.witness.org/deepfakery/?fbclid=IwAR3qokWzaU5VbxQ8VyF-PwDo0AKWxVmXT4NA-npCIciqOK4pDTEHhQKIE78#faking-powerful">Faking
                            the powerful</a></strong></em></p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2Fyff_IAWW57E%3Fstart%3D1166%26feature%3Doembed%26start%3D1166&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3Dyff_IAWW57E&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fyff_IAWW57E%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="12981788567313218">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="12981788567313218"><span class="marginnote">Bill Posters and Daniel Howe
                    (Spectre Project) and Stephanie Lepp (Deep Reckonings), in conversation with Sam Gregory
                    (WITNESS)</span></figure>
            <p>Episode 2 — <em><strong><a
                            href="https://lab.witness.org/deepfakery/?fbclid=IwAR3qokWzaU5VbxQ8VyF-PwDo0AKWxVmXT4NA-npCIciqOK4pDTEHhQKIE78#not-funny">Not
                            funny anymore: Deepfakes, manipulated media, and mis/disinformation</a></strong></em></p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FzUvMi1Uw_rA%3Fstart%3D56%26feature%3Doembed%26start%3D56&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DzUvMi1Uw_rA&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FzUvMi1Uw_rA%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="3733249804994330203">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="3733249804994330203"><span class="marginnote">Jane Lytvynenko (Buzzfeed News),
                    Karen Hao (MIT Tech Review) and Brandi Collins-Dexter (Shorenstein Center) in conversation with
                    Corin Faife (WITNESS)</span></figure>
            <p>Episode 3 — <em><strong><a
                            href="https://lab.witness.org/deepfakery/?fbclid=IwAR3qokWzaU5VbxQ8VyF-PwDo0AKWxVmXT4NA-npCIciqOK4pDTEHhQKIE78#welcome-chechnya">Using
                            AI-generated Face Doubles in Documentary: Welcome to Chechnya</a></strong></em></p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FcaMEMJyx2Bk%3Fstart%3D2044%26feature%3Doembed%26start%3D2044&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DcaMEMJyx2Bk&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FcaMEMJyx2Bk%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="6940127843324641259">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="6940127843324641259"><span class="marginnote">David France (Welcome to
                    Chechnya), in conversation with Kat Cizek (Co-Creation Studio at MIT Open Documentary Lab)</span>
            </figure>
            <p>Episode 4 — <em><strong><a
                            href="https://lab.witness.org/deepfakery/?fbclid=IwAR3qokWzaU5VbxQ8VyF-PwDo0AKWxVmXT4NA-npCIciqOK4pDTEHhQKIE78#boundary-lines">Boundary
                            lines? Deepfakes weaponized against journalists and activists</a></strong></em></p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FSC97o2XsPsI%3Fstart%3D1249%26feature%3Doembed%26start%3D1249&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DSC97o2XsPsI&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FSC97o2XsPsI%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="4227610575227348797">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="4227610575227348797"><span class="marginnote">Samantha Cole (Vice), Megha
                    Rajagopalan (BuzzFeed News) and Nina Schick (“The Coming Infocalypse”) in conversation with Assia
                    Boundaoui (Co-Creation Studio at MIT Open Documentary Lab)</span></figure>
            <p>Episode 5 — <em><strong><a
                            href="https://lab.witness.org/deepfakery/?fbclid=IwAR3qokWzaU5VbxQ8VyF-PwDo0AKWxVmXT4NA-npCIciqOK4pDTEHhQKIE78#manipulated-memories">Manipulating
                            memories: Archives, history and deepfakes</a></strong></em></p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FPQVFgqHAZ5M%3Fstart%3D1647%26feature%3Doembed%26start%3D1647&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DPQVFgqHAZ5M&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FPQVFgqHAZ5M%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="7512530575121504945">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="7512530575121504945"><span class="marginnote">Francesca Panetta and Halsey
                    Burgund (In Event of Moon Disaster), James Coupe (Thoughtworks Synthetic Media Resident 2020), and
                    Yvonne Ng (WITNESS), in conversation with William Uricchio (MIT)</span></figure>
            <p><strong>Episode 6 — </strong><em><strong><a
                            href="https://lab.witness.org/deepfakery/?fbclid=IwAR3qokWzaU5VbxQ8VyF-PwDo0AKWxVmXT4NA-npCIciqOK4pDTEHhQKIE78#satire-deepfakes">Still
                            funny?: Satire, deepfakes, and human rights globally</a></strong></em></p>
            <figure><iframe
                    src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FzOegSR5j9j0%3Fstart%3D1806%26feature%3Doembed%26start%3D1806&amp;display_name=YouTube&amp;url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DzOegSR5j9j0&amp;image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FzOegSR5j9j0%2Fhqdefault.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=youtube"
                    width="800" height="450" frameborder="0" allowfullscreen="true"></iframe><label
                    class="margin-toggle" for="11452595863111231941">&#9997;&#xFE0E;</label><input class="margin-toggle"
                    type="checkbox" id="11452595863111231941"><span class="marginnote">Julie Owono (Internet Sans
                    Frontieres), Evelyn Aswad (University of Oklahoma), and Adebayo Okewowo (WITNESS) in conversation
                    with Sam Gregory (WITNESS)</span></figure>
            <blockquote>
                <p><em>Immerse</em> is an initiative of the MIT Open DocLab and receives funding from Just Films | Ford
                    Foundation and the MacArthur Foundation. IFP is our fiscal sponsor. Learn more <a
                        href="https://immerse.news/whats-our-editorial-vision-82d7eeb3e7b9">here</a>. We are committed
                    to exploring and showcasing emerging nonfiction projects that push the boundaries of media and
                    tackle issues of social justice — and rely on friends like you to sustain ourselves and grow. <a
                        href="https://fiscal.ifp.org/project.cfm/1074/">Join us by making a gift today.</a></p>
            </blockquote>
        </section>
    </article>
</body>

</html>